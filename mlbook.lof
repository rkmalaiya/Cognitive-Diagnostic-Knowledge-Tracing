\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces {\bf The statistical machine learning framework.} The Data Generating Process (DGP) generates observable training data from the unobservable environmental probability distribution. The learning machine observes the training data and uses its beliefs about the structure of the environmental probability distribution in order to construct a best-approximating distribution of the environmental distribution.}}{4}
\contentsline {figure}{\numberline {1.2}{\ignorespaces {\bf Representation of real-world events as feature vectors.} An event in the environment of a learning machine is represented as a feature vector whose elements are real numbers. The feature vector suppresses irrelevant information and exaggerates relevant information in order to provide an appropriate abstract representation of the event.}}{5}
\contentsline {figure}{\numberline {1.3}{\ignorespaces {\bf Artificial neural network node representation of stock market linear regression predictive model.} An artificial neural network node representation of a linear regression model which predicts tomorrow's closing price $x_{t+1}$ given today's closing price $x_t$ using the formula $\mathaccentV {hat}05E{x}_{t+1}({\boldsymbol \theta }) = \theta _1 x_t + \theta _2$.}}{11}
\contentsline {figure}{\numberline {1.4}{\ignorespaces {\bf Overview of a supervised learning machine.} The data generating process generates an event which is detected as a feature vector by the supervised learning machine. The feature vector consists of two components: An input pattern vector ${\bf s}$ and a desired response vector ${\bf y}$. The learning machine uses the input pattern and its internal knowledge state specified by a parameter vector ${\boldsymbol \theta }$ to produce a response $\mathaccentV {ddot}07F{\bf y}$. A discrepancy between the learning machine's produced response $\mathaccentV {ddot}07F{\bf y}$ and the desired response ${\bf y}$ results in an update to the learning machine's internal knowledge state ${\boldsymbol \theta }$.}}{19}
\contentsline {figure}{\numberline {1.5}{\ignorespaces {\bf Network representation of a basis function approximation strategy.} The response of the network is a weighted sum of the hidden states. Each hidden state is the value of the basis function computed from the basis function parameters and the elements of the input pattern vector.}}{22}
\contentsline {figure}{\numberline {1.6}{\ignorespaces {\bf Response characteristics of a radial basis function.} A radial basis function has a symmetric exponential response curve centered at some mean $m$ with radius $\sigma $. }}{23}
\contentsline {figure}{\numberline {1.7}{\ignorespaces {\bf A weighted sum of radial basis functions can approximate an arbitrary smooth function.} In this diagram the response of an individual radial basis function is plotted using dashed lines. The sum of the responses of the three individual radial basis functions results in the construction of a new function with multiple modes.}}{24}
\contentsline {figure}{\numberline {1.8}{\ignorespaces {\bf Response characteristics of a sigmoidal basis function.} A sigmoidal function is a monotonic increasing function with lower and upper bounds.}}{24}
\contentsline {figure}{\numberline {1.9}{\ignorespaces {\bf Response characteristics of a softplus basis function.} A softplus function is a smooth approximation to a rectilinear unit whose output is equal to its input when the input is positive and whose output is zero otherwise.}}{26}
\contentsline {figure}{\numberline {1.10}{\ignorespaces {\bf Gated recurrent neural network for learning sequences.} A gated recurrent neural network for learning sequences updates the current hidden unit responses by combining the previous responses of the hidden units and the current input pattern vector. The current hidden unit responses are then used to generate a prediction which is functionally dependent upon the past history of input patterns. The diagram does not show the operation of the update gate functions and reset gate functions. }}{28}
\contentsline {figure}{\numberline {1.11}{\ignorespaces {\bf Overview of an unsupervised learning machine.} Although the training data for an unsupervised learning machine consists only of input pattern vectors, an unsupervised learning machine can still learn to generate an appropriate response for a given input pattern.}}{31}
\contentsline {figure}{\numberline {1.12}{\ignorespaces {\bf An example of a term by document matrix.} The number of times the $j$th term occurs in document $k$ is specified by a number in row $j$ of the $k$th column of the {\em term by document} matrix. Note that documents 1 and 2 are considered similar because they have more terms in common. The terms "dog" and "paper" are considered less similar because they have fewer documents in common.}}{33}
\contentsline {figure}{\numberline {1.13}{\ignorespaces {\bf Denoising autoencoder with one layer of hidden units.} An input pattern is presented to the denoising autoencoder and then corrupted by additive noise. Next the corrupted input pattern is mapped into a pattern $[h_1, h_2, h_3]$ of hidden unit states using only a relatively small number of hidden units. The small number of hidden units prevents the autoencoder from memorizing input patterns and forces the autoencoder to represent the corrupted input pattern using only statistical regularities which are dominant in the statistical environment. }}{34}
\contentsline {figure}{\numberline {1.14}{\ignorespaces {\bf Examples of Clustering Problems.} Spherical clustering problems can be solved using methods such as K-means, while more complicated ellipsoidal clustering problems can often be handled by minimizing similarity between clusters while maximizing similarity within clusters. Advanced methods such as stochastic neighborhood embedding clustering methods may be required for more challenging clustering problems where clusters are not compact.}}{39}
\contentsline {figure}{\numberline {1.15}{\ignorespaces {\bf Conditional dependency graph for a Markov Logic Net.} This Markov Logic Net (MLN) was generated by assigning each of the six propositions associated with Table \ref {Ch1:Table:MLNrules} to six nodes and then connecting two nodes with a link if they are present in any of the six grounded formulas generated from the rulebase in Table \ref {Ch1:Table:MLNrules}. Note: $\texttt {R} \equiv \texttt {ROVER}$, $\texttt {T} \equiv \texttt {TWEETY}$.}}{41}
\contentsline {figure}{\numberline {1.16}{\ignorespaces {\bf Overview of reinforcement learning in a reactive learning environment.} In an episode-based reinforcement adaptive reactive learning environment, the statistical environment generates an initial state of an episode. The learning machine then generates a response using its current parameter vector which alters the learning machine's environment. The altered statistical environment then generates the next state of the episode. The interactions of the learning machine and environment continue until the episode is concluded. The parameter values of the learning machine are then updated at the end of an episode. It is assumed that episodes are independent and identically distributed when the parameter values of the learning machine are fixed.}}{45}
\contentsline {figure}{\numberline {1.17}{\ignorespaces {\bf A simplified one-dimensional lunar lander problem.} The lunar lander module begins its descent with a downward velocity at some fixed height. A gravitational accelerative force moves the lunar lander downward at a progressively faster rate. The lunar lander has a fixed amount of fuel and can generate thrust to counteract this force. The thrust is generated as a function of the lunar lander's physical state and current parameter values. These parameter values are adjusted throughout the lunar lander's descent.}}{48}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces {\bf Examples of different real-world images of cats and dogs.} The problem of distinguishing cats from dogs might seem straightforward upon initial review but, in fact, this is a challenging problem because it is virtually impossible to identify a set of observable features which are both necessary and sufficient to distinguish these two object classes. The design of a learning machine that can distinguish between images of dogs and images of cats is a challenging task.}}{60}
\contentsline {figure}{\numberline {2.2}{\ignorespaces {\bf Representation of logical assertions using set theory.} This figure depicts logical assertions such as: (i) \texttt {EVERY DOG IS AN ANIMAL}, (ii) \texttt {NO OBJECT IS BOTH A DOG AND A CAT}, and (iii) \texttt {IF AN OBJECT IS A DOG, THEN THE THE OBJECT IS AN ANIMAL.}}}{61}
\contentsline {figure}{\numberline {2.3}{\ignorespaces {\bf Examples of different types of directed graphs.} The only graph in the figure which is not a directed acyclic graph is the directed graph with a directed cycle located in the lower right side of the figure.}}{65}
\contentsline {figure}{\numberline {2.4}{\ignorespaces {\bf An example of an undirected graph.} This figure shows an undirected graph $({\cal V}, {\cal E})$ where ${\cal V} \equiv \{1,2,3,4,5\}$ and ${\cal E} \equiv \left \{ \{1,2\}, \{1,3\}, \{3,4\}, \{3,5\}, \{4,5\} \right \}$. In addition the eleven cliques of the graph are drawn as dashed circles which identify specific groups of nodes. This example has three maximal cliques $\{1,2\}$, $\{1,3\}$, and $\{3,4,5\}$.}}{66}
\contentsline {figure}{\numberline {2.5}{\ignorespaces {\bf The preimage of a set $S_y$ under a function ${\bf f}$.} In this example, a vector-valued function ${\bf f}$ specifies a mapping from a point ${\bf x} = [x_1, x_2]$ in the domain of ${\bf f}$ into a point ${\bf y} = [y_1, y_2]$ in the range of ${\bf f}$. Let $S_y$ be a subset of points in the range of ${\bf f}$. The set of points in the domain of ${\bf f}$, $S_x$, that are mapped into $S_y$ by the function ${\bf f}$ is called the preimage of $S_y$ under ${\bf f}$.}}{68}
\contentsline {figure}{\numberline {2.6}{\ignorespaces {{\bf Illustrating the concept of a $\delta $-neighborhood of a set in a Euclidean vector space.} Example (A) illustrates a $\delta $-neighborhood of a set containing a single point. Example (B) illustrates a $\delta $-neighborhood of a set which contains all points contained in some ball. Example (C) illustrates another type of $\delta $-neighborhood of a set which is defined as the boundary of some ball.}}}{72}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces {\bf The concept of a dynamical system.} A dynamical system generates a final state ${\bf x}_F$ given four inputs: (1) an initial state ${\bf x}_0$, (2) an initial time $t_0$, (3) a final time $t_F$, and (4) an event timeline function ${\boldsymbol \xi }$ which specifies the events experienced by the machine. }}{78}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces {\bf A multilayer linear feedforward neural network is equivalent to a single layer linear feedforward neural network.} The left-hand network depicts a linear response function which generates a response pattern ${\bf y}$ given input pattern ${\bf s}$ using the formulas ${\bf y} = {\bf W}{\bf h}$ and ${\bf h} = {\bf V}{\bf s}$. The right-hand network implements a linear response function which generates a response pattern ${\bf y}$ given input pattern ${\bf s}$ using the formula ${\bf y} = {\bf Q}{\bf s}$. By choosing ${\bf Q} = {\bf W}{\bf V}$ both networks implement the same linear response function.}}{96}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces {\bf Examples of saddlepoints of an objective function on a one-dimensional parameter space.} The objective function $V : {\cal R} \rightarrow {\cal R}$ depicted in this figure provides a geometric illustration of saddlepoints as well as local minima, local maxima, and flat regions.}}{140}
\contentsline {figure}{\numberline {5.2}{\ignorespaces {\bf Examples of saddlepoints of an objective function on a two-dimensional parameter space.} The objective function $V : {\cal R}^2 \rightarrow {\cal R}$ depicted in this figure provides a geometric illustration of saddlepoints as well as local minima, local maxima, and flat regions.}}{141}
\contentsline {figure}{\numberline {5.3}{\ignorespaces {\bf Geometric interpretation of the definition of a convex function.} A convex function, $V$, is defined as a function such that the value of $V$ is less than or equal to a "linearized" version of $V$ evaluated along {\em any} line segment (e.g., dashed line in figure) connecting two points in the function's domain. }}{141}
\contentsline {figure}{\numberline {5.4}{\ignorespaces {\bf Hiking on a canyon path as an example nonlinear constrained optimization problem.} The figure depicts a contour map of a canyon whose depth reaches 1000 feet below sea level (0 feet elevation). The hiker follows a hiking path which does not visit the bottom of the canyon but reaches a maximum depth of 750 feet before the hiking path takes the hiker out of the canyon. In a typical constrained optimization machine learning problem, the hiker's location on the hiking path specifies a point in the parameter space which satisfies a specified constraint. That is, if the hiker leaves the hiking path then the constraint is violated. The depth of the canyon at the hiker's location corresponds to the objective function evaluated at the hiker's location. }}{146}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces {\bf Some examples of convergence to a set in state space.} Three trajectories of a nonlinear dynamical system initiated at three distinct distinct initial conditions labeled A, B, and C respectively in a two-dimensional state space. The trajectories with initial conditions A and B converge to a region of the state space consisting of multiple equilibrium points. The trajectory with initial condition C converges to one specific equilibrium point.}}{160}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.1}{\ignorespaces {\bf Geometric interpretation of the downhill condition in deterministic nonlinear optimization theory.} The downhill search condition requires that the angular separation $\psi $ between the search direction, ${\bf d}({\bf x})$, of the descent algorithm and the negative gradient of the objective function, $-{\bf g}({\bf x})$ be less than $90$ degrees.}}{176}
\contentsline {figure}{\numberline {7.2}{\ignorespaces {\bf Geometric interpretation of an optimal stepsize.} If $V : {\cal R}^d \rightarrow {\cal R}$ is an objective function, ${\bf x}$ is the current state of the search algorithm in state space, and let the search direction ${\bf d} \in {\cal R}^d$. Let $V_{ {\bf x}, {\bf d}} : [0,\infty ) \rightarrow {\cal R}$ be defined such that $V_{ {\bf x}, {\bf d}}(\gamma ) = V({\bf x} + \gamma {\bf d})$. An optimal stepsize, $\gamma ^*$, decreases the objective function $V_{{\bf x}, {\bf d}}$ by a maximum amount over a closed interval $[0,\gamma _{max}]$. The optimal stepsize, $\gamma ^*$, may change if $\gamma _{max}$ is changed.}}{178}
\contentsline {figure}{\numberline {7.3}{\ignorespaces {\bf Example of a descent algorithm which does not converge with constant stepsize.} If the stepsize for a descent algorithm is constant, then the descent algorithm which generate a sequence of states which will tend to ''jump over'' the desired solution resulting in a situation where the descent algorithm fails to converge. }}{179}
\contentsline {figure}{\numberline {7.4}{\ignorespaces {\bf Example of a descent algorithm which does not converge with decreasing stepsize.} If the stepsize for a descent algorithm is decreased too rapidly, then the descent algorithm might stop before it reaches a desirable solution.}}{180}
\contentsline {figure}{\numberline {7.5}{\ignorespaces {\bf Example partitions of the domain of a non-convex function.} This non-convex function has a unique global minimizer on $\Omega _1$, a global maximizer and flat on $\Omega _2$, and a global maximizer and global minimizer on $\Omega _3$. }}{186}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {8.1}{\ignorespaces {\bf Comparison of the Riemann integral with the Lebesgue integral.} Riemann integral covers the area under the curve using small rectangles with some small fixed width, while the Lebesgue integral covers the area under the curve using small rectangles of varying widths.}}{217}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {9.1}{\ignorespaces {\bf Graphical illustration of convergence with probability one.} Let $S$ be the set of realizations of a stochastic sequence which converge and the complement of $S$, $\neg S$, be the set of realizations which do not converge. Convergence with probability one means that the probability that a realization is in $S$ is exactly equal to one and the probability that a realization is in $\neg S$ is exactly equal to zero. }}{236}
\contentsline {figure}{\numberline {9.2}{\ignorespaces {\bf Graphical illustration of convergence in mean square.} Convergence in mean square means that that the expected variation of a trajectory from its destination tends to zero as time increases.}}{239}
\contentsline {figure}{\numberline {9.3}{\ignorespaces {\bf Graphical illustration of the concept of convergence in probability.} Convergence in probability means that for a ball of given radius centered at the trajectory's destination, the probability that a trajectory is in that ball tends to one as time increases.}}{240}
\contentsline {figure}{\numberline {9.4}{\ignorespaces {\bf Relationships among types of stochastic convergence.} Deterministic convergence implies convergence with probability one. Either Convergence in mean-square or convergence with probability one implies convergence in probability. Convergence in probability implies convergence in distribution. Convergence in probability for a bounded stochastic sequence implies convergence in mean-square. }}{243}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {10.1}{\ignorespaces {\bf Chi-squared density for different choices of degrees of freedom.} When the degrees of freedom are small then the chi-squared probability density is similar in form to an exponential probability density. When the degrees of freedom are large then the chi-square probability density is similar in form to a Gaussian probability density. The chi-squared density is a special case of the gamma density function when the shape parameter is chosen to be one-half of the degrees of freedom of the chi-squared density and the rate parameter is chosen to be equal to $2$. }}{257}
\contentsline {figure}{\numberline {10.2}{\ignorespaces {\bf Hyperbolic secant density as a function of its standard deviation.} This is a useful probability density since it closely approximates the Laplace probability density when the standard deviation is chosen to be sufficiently small.}}{259}
\contentsline {figure}{\numberline {10.3}{\ignorespaces {\bf Bayes network representation of a first-order Markov chain.} Conditional independence relationships between the random variables in a first-order Markov chain may be represented as a conditional dependency graph in which every node has both a parent node and child node with the possible exception of the end nodes.}}{263}
\contentsline {figure}{\numberline {10.4}{\ignorespaces {\bf A Bayesian network for representing knowledge for a medical diagnosis problem.} Socio-economic status influences the probability that an individual will smoke and additionally influences the probability that an individual will be exposed to asbestos. The probability of an individual acquiring lung cancer is influenced by genetic factors, the individual's history of smoking, and the extent to which the individual has been exposed to asbestos.}}{264}
\contentsline {figure}{\numberline {10.5}{\ignorespaces {\bf A Bayesian network representation of a linear or nonlinear regression model.} A linear or nonlinear regression model is a Bayesian network where the predictor random variables ${\bf \mathaccentV {tilde}07E{s}}_1, \ldots , {\bf \mathaccentV {tilde}07E{s}}_d$ are the causal antecedents of the response random variables ${\bf \mathaccentV {tilde}07E{y}}_1, \ldots , {\bf \mathaccentV {tilde}07E{y}}_m$.}}{266}
\contentsline {figure}{\numberline {10.6}{\ignorespaces {\bf A Bayesian network representation of a hidden Markov model.} The random variables ${\bf \mathaccentV {tilde}07E{h}}_1$, ${\bf \mathaccentV {tilde}07E{h}}_2$, and ${\bf \mathaccentV {tilde}07E{h}}_3$ are not directly observable and correspond to ``hidden system states''. The probability distribution of ${\bf \mathaccentV {tilde}07E{h}}_{t+1}$ is functionally dependent only upon ${\bf \mathaccentV {tilde}07E{h}}_{t+1}$. Realizations of the observable random variables ${\bf \mathaccentV {tilde}07E{o}}_1$, ${\bf \mathaccentV {tilde}07E{o}}_2$, and ${\bf \mathaccentV {tilde}07E{o}}_3$ are observable. The probability distribution of an observable random variable ${\bf \mathaccentV {tilde}07E{o}}_t$ is functionally dependent only upon its corresponding hidden counterpart ${\bf \mathaccentV {tilde}07E{h}}_t$. }}{267}
\contentsline {figure}{\numberline {10.7}{\ignorespaces {\bf Markov random field representation of medical knowledge.} This figure depicts the conditional dependency graph for a Markov random field representing the probabilistic knowledge representation depicted by the Bayes net in Figure \ref {fig:Ch10:BayesNetMedical} {\em plus} additional knowledge that genetic dispositions can influence smoking behaviors. This additional knowledge that the probability of smoking is functionally dependent upon genetic disposition is modeled by allowing the probability distribution of $\mathaccentV {tilde}07E{x}_2$ (smoking) to be dependent upon the random variable $\mathaccentV {tilde}07E{x}_5$ (genetic disposition). Allowing this additional probabilistic constraint results in a conditional dependency graph which can not be naturally represented as a Bayesian network since the graph is not a directed acyclic graph. However, these constraints are naturally represented as a conditional dependency graph for a Markov random field (MRF). In particular, this figure represents the conditional dependency graph for the MRF using two maximal cliques: $\{ \mathaccentV {tilde}07E{x}_2, \mathaccentV {tilde}07E{x}_5, \mathaccentV {tilde}07E{x}_4 \}$ and $\{ \mathaccentV {tilde}07E{x}_1, \mathaccentV {tilde}07E{x}_2, \mathaccentV {tilde}07E{x}_3, \mathaccentV {tilde}07E{x}_4 \}$. In the MRF framework, the probability distribution of a random variable in the MRF is functionally dependent only upon the random variables which are connected to that random variable on the undirected MRF conditional dependency graph. }}{278}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {11.1}{\ignorespaces {\bf An example of a transition matrix for a first-order Markov chain with three states.} Each random variable in the first-order Markov chain has three states: $\text {Sunny}$, $\text {Rainy}$, and $\text {Cloudy}$. The transition matrix in this example specifies the probability that tomorrow's weather will be $\text {Rainy}$ given today's weather is $\text {Sunny}$. So, for example, according to the transition matrix if today is a $\text {Sunny}$ then the probability that tomorrow's weather will remain $\text {Sunny}$ is 0.2, the probability that tomorrow's weather will be $\text {Cloudy}$ is 0.6, and the probability that tomorrow's weather will be $\text {Rainy}$ will be 0.2. A graphical representation of this information is provided by the state transition graph which is depicted in Figure \ref {fig:Ch11:3stategraph}.}}{289}
\contentsline {figure}{\numberline {11.2}{\ignorespaces {\bf An example of a state transition graph for a first-order Markov chain with three states.} The state transition graph provides a method for graphically illustrating the information in the transition matrix for the first-order Markov chain (see Figure \ref {fig:Ch11:3stateTransitionMx})}}{290}
\contentsline {figure}{\numberline {11.3}{\ignorespaces {\bf Examples of first-order Markov chain state transition graphs which are either irreducible or aperiodic.} Some additional comments.}}{292}
\contentsline {figure}{\numberline {11.4}{\ignorespaces {\bf Shape of Gibbs density as a function of the temperature parameter ${\cal T}$.} When the temperature parameter ${\cal T}$ is close to infinity, the Gibbs density is uniform over the sample space. When the temperature parameter ${\cal T}$ is close to zero, the Gibbs density is uniform over the set of most probable points in the sample space. Typically, it is assumed that ${\cal T} = 1$ corresponds to the undistorted shape of the Gibbs density.}}{296}
\contentsline {figure}{\numberline {11.5}{\ignorespaces The presence of low probability regions surrounding a high probability region of interest can dramatically slow down stochastic search. }}{298}
\contentsline {figure}{\numberline {11.6}{\ignorespaces {\em The restricted Boltzmann machine.} The restricted Boltzmann machine consists of one layer of input units and one layer of hidden units. Each hidden unit computes a logistic sigmoidal function of a weighted sum of a subset of input unit states. Each input unit computs a logistic sigmoidal function of a weighted sum of a subset of hidden unit states. In addition, the connection weight from input unit $j$ to hidden unit $k$ is constrained to be always equal to the connection weight from hidden unit $k$ to input unit $j$.}}{307}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {12.1}{\ignorespaces A typical stepsize annealing schedule for a stochastic approximation algorithm that satisfies Equation (\ref {ch11:stepsizenot2large}) and Equation (\ref {ch11:stepsizenot2small}). The stepsize is constant with value $\gamma _0$ until the time index reaches time $T_0$. At that point, the stepsize gradually decreases with a half-life of $\tau $. By selecting different choices for $\gamma _0$, $T_0$, and $\tau $ a variety of different annealing schedules can be generated. }}{317}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {13.1}{\ignorespaces Objective Functions for Learning With Respective Global Minimizers for Different Numbers of Training Stimuli.}}{346}
\contentsline {figure}{\numberline {13.2}{\ignorespaces {\bf Example of Besag's Coding Assumption.} Each pixel value in this image is interpreted as a realization of a random variable. The conditional density of the random pixels in a dashed circle is functionally dependent only upon the values of the pixels located outside the dashed circle yet inside the square that contains the dashed circle. If two squares overlap, this indicates that the random pixels in the dashed circles are not statistically independent. In this case, since none of the neighborhoods of the random pixels overlap, the likelihood of observing the pixels associated with these neighborhoods can be calculated by simply multiplying together the conditional densities for the random pixels in the dashed circles which are conditioned upon the pixels outside the dashed circle but inside the square surrounding the circle.}}{367}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {14.1}{\ignorespaces {\bf The estimated parameters of a linear regression model which predicts values of Y from values of X will be different for two different data sets even if the data sets are generated from the same data generating process.}}}{380}
\contentsline {figure}{\numberline {14.2}{\ignorespaces {\bf A model which has a good fit to the data generating process.}As sample size increases, the fit of the model to both the training and the test data will tend to decrease in situations where no over-fitting is present. Note that the fit of the model to the test data set is typically slightly worse than the fit of the model to the training data set. }}{381}
\contentsline {figure}{\numberline {14.3}{\ignorespaces {\bf A model which fits the training data but exhibits an overfitting phenomenon.} As the sample size increases, performance on the training and test data sets initially tends to decrease but at a certain point the model fit on the training data will continue to decrease but the model fit on the test data will begin to increase.}}{381}
\contentsline {figure}{\numberline {14.4}{\ignorespaces Frequency histogram of observed values of a statistic ${\bf \mathaccentV {hat}05E{s}}_n$ for a fixed sample size $n$. The frequency histogram approximates the probability density of the statistic. In addition, the mean, ${\bf \mathaccentV {bar}016{s}}_n$, and standard deviation, $\sigma _{ {\bf \mathaccentV {hat}05E{s}}_n}$, of the observed values of the statistic. of the frequency histogram can be used to provide estimates of the average value of the statistic and the statistic's sampling error respectively}}{384}
\contentsline {figure}{\numberline {14.5}{\ignorespaces {\bf Estimating the sampling error of a statistic by collecting additional data.}. This figure depicts a methodology for estimating the sampling error for a statistic computed for a sample of size $n$. This is achieved by collecting data for $m$ data sets such that each data set consists of exactly $n$ data records. A statistic is then computed for each of the $m$ data sets. The mean of the $m$ statistics provides an improved estimator of the expected value of the original statistic. The standard deviation of the $m$ statistics provides an estimate of the statistic's sampling error.}}{389}
\contentsline {figure}{\numberline {14.6}{\ignorespaces {\bf Estimating the sampling error of a statistic using a bootstrap methodology.}. This figure depicts the bootstrap methodology for estimating the sampling error for a statistic computed for a sample of size $n$. A bootstrap probability distribution is estimated which is presumed to specify the probability distribution of an observation. Next, $m$ data sets are sampled from the bootstrap probability distribution. A statistic is then computed for each of the $m$ data sets. The mean of the $m$ statistics provides an improved estimator of the expected value of the original statistic. The standard deviation of the $m$ statistics provides an estimate of the statistic's sampling error.}}{391}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {15.1}{\ignorespaces Two examples of how the condition number as a function of sample size may change. If the condition number is converging to a finite number, this is empirical evidence consistent with Assumption A9. If the condition number is diverging to infinity, this indicates a violation of Assumption A9. Note that if the largest eigenvalue of ${\bf \mathaccentV {hat}05E{A}}_n$ is numerically close to zero, then the condition number for ${\bf \mathaccentV {hat}05E{A}}_n$ is defined as infinity.}}{402}
\contentsline {figure}{\numberline {15.2}{\ignorespaces {\bf Checking the asymptotic formula for standard errors of the parameter estimates.} The sampling error for the parameter estimates of a logistic regression model were estimated (see Example ***) using an increasing number of bootstrap data samples. As the number of bootstrap data samples becomes large, the estimated sampling error and the theoretically derived formula for the sampling error (see Box ***) converge.}}{411}
\contentsline {figure}{\numberline {15.3}{\ignorespaces {\bf Ellipsoidal Confidence Regions.} This figure depicts an ellipsoidal confidence region $\{ {\bf x} \in {\cal R}^2 : {\bf x}^T {\bf C}^{-1} {\bf x} \leq K^2 \}$ where ${\bf C}$ is a positive definite diagonal matrix with eigenvalues $\lambda _1$ and $\lambda _2$.}}{416}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {16.1}{\ignorespaces {\bf The concept of a cross-validation risk model selection criterion.} A Cross Validation Risk MSC estimates the expected empirical risk on a test data set using parameter values estimated from a training data set. Equivalently, the cross-validation risk MSC estimates test data performance for the density in the model which best-fits the training data.}}{426}
\contentsline {figure}{\numberline {16.2}{\ignorespaces {\bf The concept of a Bayesian model selection criterion.} A Bayesian MSC is computed using a weighted average of the likelihood of each density in the probability model given the observed data. The weights in the weighted average are specified by the model's parameter prior.}}{427}
\contentsline {figure}{\numberline {16.3}{\ignorespaces {\bf The concept of a misspecification detection model selection criterion.} A misspecification detection model selection criterion is a binary-valued function which is used to decide whether or not model misspecification is present. A probability model is represented by a set of probability distributions which are represented as small circles. If the data generating process density (which is represented by a circle containing a star) is not inside the probability model this corresponds to the case of model misspecification. In such a situation, the misspecification detection MSC should return the value of one. If misspecification is not present, the misspecification detection MSC should return the value of zero.}}{427}
\contentsline {figure}{\numberline {16.4}{\ignorespaces {\bf Checking the asymptotic formula for estimating overfitting bias.} The parameter estimates of a logistic regression model were estimated using an increasing number of bootstrap data samples on both the training data which was used to train the regression model and a novel test data set. A nonparametric bootstrap methodology was used to estimate the difference between the average empirical risk on the test data set and the average empirical risk on the training data set. This estimated average bias was then used to estimate the average empirical risk of the logistic regression model on a test data set (see Example *** for specific details of simulation study). As the number of bootstrap data samples becomes large, the estimated empirical risk on the test data computed using the nonparametric bootstrap simulation methodology converges to the theoretically derived formula for the estimated empirical risk.}}{432}
